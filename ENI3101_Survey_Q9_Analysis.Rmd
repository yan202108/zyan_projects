---
title: "ENI3101_Survey_Q9_Anylysis"
author: "Yan Zhou"
date: "14/12/2021"
output: html_document
---
Project Goal: Exact the common words from the answers of Q9
```{r}
library(tm)
library(tidyverse)
library(wordcloud)
library(tidytext)
library(dplyr)

```
Import the data file
```{r}
#setwd("~/Desktop/MSIS/LA/Texure project")
df<-read.csv("Q9_rawdata.csv")
df<-df %>% mutate(story="total" )
sherlock<-df %>% mutate(story = factor(story, levels = unique(story)))
```
Tokenized data and remove the stop words
```{r}
library(Hmisc)
removed_words<-c("1","2","people","feel",
                 "personality","coworkers","tend")
                     
tidy_sherlock <- sherlock %>%
  mutate(line = row_number()) %>%
  unnest_tokens(word, Q9) %>%
  anti_join(stop_words) %>%
  filter(word %nin% removed_words) 
       
```
Show the word's occurrence in order

```{r}
tidy_sherlock %>%
  count(word, sort = TRUE)
```
plot the words of top 20
```{r}
library(ggplot2)
sherlock_tf_idf <- tidy_sherlock %>%
  count(story, word, sort = TRUE) 
sherlock_n<-sherlock_tf_idf[1:20,]
sherlock_n_plot<- sherlock_n %>% 
  ggplot(aes(reorder(word, n),n,color= word)) +
  geom_bar(stat="identity", width=.5,fill = "white",show.legend = FALSE) +
  coord_flip() 
sherlock_n_plot
```
#wordcloud
```{r}
cleaned_tokens<-data_frame(word=tidy_sherlock$word)
cleaned_tokens
# Define a color palette
pal <- brewer.pal(8,"Dark2")

# Plot the 100 most common words
cleaned_tokens %>%
  count(word) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors=pal))

```

wordcloud2
```{r}
word_cloud_generator <- function(data) { 

docs<- VCorpus(VectorSource(data))

#Specify stopwords other than in-bult english stopwords
skipwords = c("the","and","that","when","how","did","was","why","what","for","can","with","have","public",
              "work", "career", "careers" ,stopwords("en"))


# Simple Transformation
for (j in seq(docs))
{
  docs[[j]] = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", docs[[j]])
  docs[[j]] = gsub("@\\w+", "", docs[[j]])
  docs[[j]] = gsub("http\\w+", "", docs[[j]])
  docs[[j]] = gsub("[ \t]{2,}", "", docs[[j]])
  docs[[j]] = gsub("^\\s+|\\s+quot;", "", docs[[j]])
  docs[[j]] = gsub("[^\x20-\x7E]", "", docs[[j]])
}

kb.tf <- list(weighting = weightTf, 
              removePunctuation = TRUE,
              stopwords = skipwords,
              tolower = TRUE,
              minWordLength = 4,
              removeNumbers = TRUE, stripWhitespace = TRUE,
              stemDocument= TRUE)

# term-document matrix
docs <- tm_map(docs, PlainTextDocument) 
tdm = TermDocumentMatrix(docs, control = kb.tf)
# convert as matrix
tdm = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(tdm), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)

# Keep wordcloud the same
set.seed(123)

wordcloud(dm$word, dm$freq, random.order=FALSE, 
          colors=brewer.pal(6, "Dark2"), min.freq=5, 
          scale=c(6,.4),rot.per=.35,max.words=150)
}
```
word cloud_2
```{r, warning = FALSE}
word_cloud_generator(df$Q9)
```




